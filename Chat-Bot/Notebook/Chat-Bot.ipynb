{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chat-Bot.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1aPAGAqyDdHR1reOEY_956u8CkCmmrGQW","authorship_tag":"ABX9TyMMompZNR4BLMNgLD5X/Tas"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nswQpvw6cM-9"},"source":["# <b>CHAT-BOT- Seq2Seq Model</b>\n","A chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent. Designed to convincingly simulate the way a human would behave as a conversational partner, chatbot systems typically require continuous tuning and testing, and many in production remain unable to adequately converse or pass the industry standard Turing test. The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot) in 1994 to describe these conversational programs. \n","![alt text](https://static.vecteezy.com/system/resources/previews/000/343/481/non_2x/chatbot-write-answer-to-messages-in-the-chat-bot-consultant-is-free-to-help-users-in-your-phone-online-vector-cartoon-illustration.jpg)\n","\n","The basic foundation of chatbots is providing the best response of any query that it receives. The best response like answering the sender questions, providing sender relevant information, ask follow-up questions and do the conversation in realistic way.\n","The chatbot needs to be able to understand the intentions of the sender’s message, determine what type of response message (a follow-up question, direct response, etc.) is required, and follow correct grammatical and lexical rules while forming the response.<br>\n","<br>\n","<b>What is Seq2Seq Model?</b><br>\n","Seq2Seq is a machine learning architecture based on the encoder-decoder paradigm. It is widely used for tasks such as translation, Q&A and other cases where it is desirable to produce a sequence from another. The main idea is to have one model, for example an RNN, which can create a good representation of the input sequence. We will refer to this model as the ‘encoder’. Using this representation, another model, the ‘decoder’, produces the expected output sequence."]},{"cell_type":"markdown","metadata":{"id":"VBy4dcIVhox2"},"source":["## <b>About the Data</b>\n","### <b>Cornell Movie--Dialogs Corpus</b>\n","#### <b>Description</b>\n","This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n","\n","- 220,579 conversational exchanges between 10,292 pairs of movie characters\n","\n","- involves 9,035 characters from 617 movies\n","\n","- in total 304,713 utterances\n","In all files the field separator is \" +++$+++ \".<br>\n","\n","#### <b>movie_lines.txt</b>\n","- contains the actual text of each utterance\n","- fields:\n","  - lineID\n","  - characterID (who uttered this phrase)\n","  - movieID\n","  - character name\n","  - text of the utterance\n","\n","#### <b>movie_conversations.txt</b>\n","- the structure of the conversations\n","- fields\n","  - characterID of the first character involved in the conversation\n","  - characterID of the second character involved in the conversation\n","  - movieID of the movie in which the conversation occurred\n","  - list of the utterances that make the conversation, in chronological order: ['lineID1','lineID2',...,'lineIDN']\n","has to be matched with movie_lines.txt to reconstruct the actual content"]},{"cell_type":"code","metadata":{"id":"0wrzCHZTzj2c","executionInfo":{"status":"ok","timestamp":1602262269591,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Importing some libraries\n","import numpy as np\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"41CU_8E0pSfn","executionInfo":{"status":"ok","timestamp":1602262269594,"user_tz":-330,"elapsed":788,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"ecf29137-f52d-4415-b023-0a205c88f613","colab":{"base_uri":"https://localhost:8080/","height":205}},"source":["### Reading the data as list\n","lines = open('/content/drive/My Drive/Chat-Bot Project/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n","lines[:10]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n"," 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n"," 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n"," 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n"," \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n"," 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n"," \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n"," 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n"," 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n"," 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"-YPXpXCqrrW8","executionInfo":{"status":"ok","timestamp":1602262271692,"user_tz":-330,"elapsed":1266,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"e2ef3e0b-2ba5-4994-abc7-04edd5bf2e2d","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["conversation = open('/content/drive/My Drive/Chat-Bot Project/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n","conversation[:10]"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n"," \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"R4PvaZcvkQlR"},"source":["### <b>Data Preprocessing</b>\n","In the data preprocessing process we would first filter the data out from unnecessary ids and separators.\n","<br>\n","<b>Step 1: Data Retrieving</b><br>\n","Here we will create a list of dialogue exchange id list all over from the conversation list.\n","<br>\n","After retrieving the dialogue exchange id list from the conversation list, I will create a dictionary to map the conversation lines with their respective ids."]},{"cell_type":"code","metadata":{"id":"Key1PTvgsN0T","executionInfo":{"status":"ok","timestamp":1602262275521,"user_tz":-330,"elapsed":1538,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"4031a51f-c06b-4d73-cad9-5d763ec70919","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["exchange_list= []\n","for conver in conversation:\n","  exchange_list.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\",\",\"\").split())\n","exchange_list[:10]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['L194', 'L195', 'L196', 'L197'],\n"," ['L198', 'L199'],\n"," ['L200', 'L201', 'L202', 'L203'],\n"," ['L204', 'L205', 'L206'],\n"," ['L207', 'L208'],\n"," ['L271', 'L272', 'L273', 'L274', 'L275'],\n"," ['L276', 'L277'],\n"," ['L280', 'L281'],\n"," ['L363', 'L364'],\n"," ['L365', 'L366']]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"IafNxfXGtgN5","executionInfo":{"status":"ok","timestamp":1602262293035,"user_tz":-330,"elapsed":1355,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"dc6be525-2de1-4216-d6eb-ac4db6605c3d","colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["### Creating a dictionary with key as id and dialogues as their values\n","dialogues = {}\n","for line in lines:\n","  dialogues[line.split(' +++$+++ ')[0]]= line.split(' +++$+++ ')[-1]\n","\n","val_cnt=0\n","for ind in dialogues:\n","  print('{}: \"{}\"'.format(ind,dialogues[ind]))\n","  val_cnt+=1\n","  if val_cnt==20:\n","    break"],"execution_count":7,"outputs":[{"output_type":"stream","text":["L1045: \"They do not!\"\n","L1044: \"They do to!\"\n","L985: \"I hope so.\"\n","L984: \"She okay?\"\n","L925: \"Let's go.\"\n","L924: \"Wow\"\n","L872: \"Okay -- you're gonna need to learn how to lie.\"\n","L871: \"No\"\n","L870: \"I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\"\n","L869: \"Like my fear of wearing pastels?\"\n","L868: \"The \"real you\".\"\n","L867: \"What good stuff?\"\n","L866: \"I figured you'd get to the good stuff eventually.\"\n","L865: \"Thank God!  If I had to hear one more story about your coiffure...\"\n","L864: \"Me.  This endless ...blonde babble. I'm like, boring myself.\"\n","L863: \"What crap?\"\n","L862: \"do you listen to this crap?\"\n","L861: \"No...\"\n","L860: \"Then Guillermo says, \"If you go any lighter, you're gonna look like an extra on 90210.\"\"\n","L699: \"You always been this selfish?\"\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F9MAkbUKmtn3"},"source":["<b>Step 2: Data Reshaping</b><br>\n","In this process we need to create two separate lists, one for questions and other for answers. The `questions` and `answers` list formation can be explained as follows:<br>\n","The first dialogue of a particular conversation will act as the question for another person, and the next dialogue/reply of another person will be the answer for the first dialogue as well as it will act as a question for upcoming dialogue by any other person or maybe the first person. This is how a conversation between two or more people works. This cycle goes on until the conversation ends."]},{"cell_type":"code","metadata":{"id":"aJ7GNKXwvJRL","executionInfo":{"status":"ok","timestamp":1602259188732,"user_tz":-330,"elapsed":4381,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"7bb72b1c-66c5-4d19-8ef2-b30d340612a5","colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["### Creating list of questions and answers\n","questions = []\n","answers = []\n","for conver in exchange_list:\n","  for i in range(len(conver)-1):\n","      questions.append(dialogues[conver[i]])\n","      answers.append(dialogues[conver[i+1]])\n","\n","print(np.shape(questions))\n","print(np.shape(answers))\n","questions[:10]"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(221616,)\n","(221616,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n"," \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n"," 'Not the hacking and gagging and spitting part.  Please.',\n"," \"You're asking me out.  That's so cute. What's your name again?\",\n"," \"No, no, it's my fault -- we didn't have a proper introduction ---\",\n"," 'Cameron.',\n"," \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n"," 'Why?',\n"," 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n"," 'Gosh, if only we could find Kat a boyfriend...']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"iPRybko3zIgj","executionInfo":{"status":"ok","timestamp":1602259188732,"user_tz":-330,"elapsed":4056,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"526c1309-36c8-4ad1-892a-77ca46e306ff","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["answers[:10]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n"," 'Not the hacking and gagging and spitting part.  Please.',\n"," \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n"," 'Forget it.',\n"," 'Cameron.',\n"," \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n"," 'Seems like she could get a date easy enough...',\n"," 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n"," \"That's a shame.\",\n"," 'Let me see what I can do.']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"arCLW3bno-Rc"},"source":["<b>Step 3: Cutting down data size</b><br>\n","As constrained to the computation limitatons, we need to chop down the data on the basis of length of the question. We are going to consider only those questions whose length is below 13. I know it is pretty short, but I dont have enough RAM to compute such a huge amount of data all at once.<br>\n","So further from here we will be using only those conversation(`sorted_ques` and `sorted_ans`)whose length of the question is below 13.\n"]},{"cell_type":"code","metadata":{"id":"pFZi0wkt0D8y","executionInfo":{"status":"ok","timestamp":1602259188733,"user_tz":-330,"elapsed":3479,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"f829d321-08b7-4154-d8c5-7037b09402f3","colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["### Creating a list of sorted questions and answers(questions wih length less than 13)\n","sorted_ques=[]\n","sorted_ans=[]\n","for i in range(len(questions)):\n","  if len(questions[i])<13:\n","    sorted_ques.append(questions[i])\n","    sorted_ans.append(answers[i])\n","print(\"Size of Sorted questions: {}\".format(np.size(sorted_ques)))\n","print(\"Size of Sorted answers: {}\".format(np.size(sorted_ans)))\n","sorted_ques[:10]"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Size of Sorted questions: 31416\n","Size of Sorted answers: 31416\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Cameron.',\n"," 'Why?',\n"," 'There.',\n"," 'Sure have.',\n"," 'Hi.',\n"," 'I was?',\n"," 'Well, no...',\n"," 'But',\n"," 'What crap?',\n"," 'No']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"1p9R_wmYrgMG"},"source":["<b>Step 4: Data Cleaning</b><br>\n","A regular expression, often abbreviated to regex, is a method of using a sequence of characters to define a search to match strings, i.e. “find and replace”-like operations.<br>\n","This step is required because there are many words used in english language which is used as their short-form for example:<br>\n","He's for He is<br>\n","'d for would<br>\n","'ll for will<br>...etc.<br>\n","\n","So here we will be changing all such words into their respective standard words using the <b>\"Regular Expression(re)\"</b> Library.\n","we will be also converting all the uppercase letters into lowercase, then we will remove all the puncuation marks from our data.<br>\n","If we don't change all these words, our model will interpret them as two different words, which is not desirable. Puncuation marks are not required because our bot won't understand them.<br>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"GZPXsT3637yq","executionInfo":{"status":"ok","timestamp":1602259188733,"user_tz":-330,"elapsed":2949,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["# Importing Regular Expression Library(re)\n","import re\n","# Creating a function to clean the text (removing punctuation marks)\n","def clean_text(txt):\n","  txt = txt.lower()\n","  txt = re.sub(r\"i'm\", \"i am\", txt)\n","  txt = re.sub(r\"he's\", \"he is\", txt)\n","  txt = re.sub(r\"she's\", \"she is\",txt)\n","  txt = re.sub(r\"that's\", \"that is\", txt)\n","  txt = re.sub(r\"what's\", \"what is\", txt)\n","  txt = re.sub(r\"where's\", \"where is\", txt)\n","  txt = re.sub(r\"\\'ll\", \" will\", txt)\n","  txt = re.sub(r\"\\'ve\", \" have\", txt)\n","  txt = re.sub(r\"\\'re\", \" are\", txt)\n","  txt = re.sub(r\"\\'d\", \" would\", txt)\n","  txt = re.sub(r\"won't\", \"will not\", txt)\n","  txt = re.sub(r\"can't\", \"can not\", txt)\n","  txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n","  return txt\n","  "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"_5B9uStn7FfT","executionInfo":{"status":"ok","timestamp":1602259189202,"user_tz":-330,"elapsed":3189,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"8faef289-8715-40fc-b6b9-f91b6c81a604","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["# Creating a list of clean questions and clean answers\n","clean_ques=[]\n","clean_ans=[]\n","for line in sorted_ques:\n","  clean_ques.append(clean_text(line))\n","for line in sorted_ans:\n","  clean_ans.append(clean_text(line))\n","clean_ques[:10]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cameron',\n"," 'why',\n"," 'there',\n"," 'sure have',\n"," 'hi',\n"," 'i was',\n"," 'well no',\n"," 'but',\n"," 'what crap',\n"," 'no']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"R56R0-sDvwhR"},"source":["Along with the questions we will also cut down the length for the answers for the same reason, and faster computation."]},{"cell_type":"code","metadata":{"id":"C1u2EMDH7_6-","executionInfo":{"status":"ok","timestamp":1602259189203,"user_tz":-330,"elapsed":2710,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"5134cb4a-6d30-4e22-b7c7-3c5519f5979f","colab":{"base_uri":"https://localhost:8080/","height":353}},"source":["# Cutting down long size answers\n","for i in range(len(clean_ans)):\n","  clean_ans[i]= ' '.join(clean_ans[i].split()[:15])\n","clean_ans[:20]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the thing is cameron i am at the mercy of a particularly hideous breed of',\n"," 'unsolved mystery she used to be really popular when she started high school then it',\n"," 'where',\n"," 'i really really really wanna go but i can not not unless my sister goes',\n"," 'looks like things worked out tonight huh',\n"," 'you never wanted to go out with me did you',\n"," 'then that is all you had to say',\n"," 'you always been this selfish',\n"," 'me this endless blonde babble i am like boring myself',\n"," 'okay you are gonna need to learn how to lie',\n"," 'lets go',\n"," 'i hope so',\n"," 'they do not',\n"," 'you might wanna think about it',\n"," 'joey',\n"," 'would you mind getting me a drink cameron',\n"," 'expensive',\n"," 'its a gay cruise line but i will be like wearing a uniform and stuff',\n"," 'my agent says i have got a good shot at being the prada guy next',\n"," 'you are concentrating awfully hard considering its gym class']"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"LGD82wHawL7g"},"source":["There are several words in our dialogue list(`clean_ques` and `clean_ans`) which does not appear very frequently, like names of some place, person or thing. We need to remove all those words whose length is lesser tham=n some threshold value, because it won't be helping our model to generalise efficiently.<br>\n","Here we can see that there are 17713 different words available in our cleaned dialogues list, and it reduced to 3467 after removing the less repeating words(here it is less than 5 times)."]},{"cell_type":"code","metadata":{"id":"RBCfnDNw9KXR","executionInfo":{"status":"ok","timestamp":1602259189203,"user_tz":-330,"elapsed":2228,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"c7eed051-627e-41b4-e23c-381922a86f51","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["### Creating a word count dictionary for the counts of different words\n","word_count ={}\n","for line in clean_ques:\n","  for word in line.split():\n","    if word not in word_count:\n","      word_count[word]=1\n","    else:\n","      word_count[word]+=1\n","\n","for line in clean_ans:\n","  for word in line.split():\n","    if word not in word_count:\n","      word_count[word]=1\n","    else:\n","      word_count[word]+=1\n","\n","print(len(word_count))\n"],"execution_count":12,"outputs":[{"output_type":"stream","text":["17713\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bh4bp-4l_pux","executionInfo":{"status":"ok","timestamp":1602259189729,"user_tz":-330,"elapsed":2531,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"fabda94f-89d6-4c8f-ebb1-cf909651adeb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Removing words whose count is below threshold value\n","threshold= 5\n","# Creating a new word wise dictionary/vocabulary to store filtered data\n","vocab = {}\n","cnt =0;\n","for word,count in word_count.items():\n","  if count>=threshold:\n","    vocab[word]=cnt\n","    cnt+=1\n","print(len(vocab))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["3467\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F0sIt_ZPzq7B"},"source":["<b> Step 4: Solving the possible variation between the input and output sequence sizes</b><br>\n","One issue to be confronted when choosing a seq2seq approach is the possible variation between the input and output sequence sizes. To handle that, we can introduce the SOS (Start Of Sequence) and EOS (End Of Sequence) tokens. By adding the EOS token at the end of each input we provide a consistent signal, facilitating the system’s capacity of learning how to finish the creation of the new sequence. We can then ask the decoder to give as many tokens as it wants until it raises the EOS token to signal the end of the output sequence."]},{"cell_type":"code","metadata":{"id":"A9PoalzsBaqP","executionInfo":{"status":"ok","timestamp":1602259189730,"user_tz":-330,"elapsed":2070,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"549c86c5-e4c3-4677-e61b-35e00c05796c","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["# Adding <SOS> and <EOS> at begining and end of string\n","for i in range(len(clean_ans)):\n","  clean_ans[i]= '<SOS> ' + clean_ans[i] + ' <EOS>'\n","clean_ans[:10]"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<SOS> the thing is cameron i am at the mercy of a particularly hideous breed of <EOS>',\n"," '<SOS> unsolved mystery she used to be really popular when she started high school then it <EOS>',\n"," '<SOS> where <EOS>',\n"," '<SOS> i really really really wanna go but i can not not unless my sister goes <EOS>',\n"," '<SOS> looks like things worked out tonight huh <EOS>',\n"," '<SOS> you never wanted to go out with me did you <EOS>',\n"," '<SOS> then that is all you had to say <EOS>',\n"," '<SOS> you always been this selfish <EOS>',\n"," '<SOS> me this endless blonde babble i am like boring myself <EOS>',\n"," '<SOS> okay you are gonna need to learn how to lie <EOS>']"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"avo6JJ4u0174"},"source":["We also need to add tokens for `<SOS>` and `<EOS>` in our vocabulary, along with this two we will also be adding two more tokens: `<PAD>` and `<OUT>`, `<PAD>` token is used to represent the padding in the string(which will be provided further). `<OUT>` token represents those words(entered from user end) which will not be available in our dictionary."]},{"cell_type":"code","metadata":{"id":"B0p_b1IhD9Eq","executionInfo":{"status":"ok","timestamp":1602259189730,"user_tz":-330,"elapsed":1605,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating other necessary tokens\n","tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n","x = len(vocab)\n","for token in tokens:\n","  vocab[token]=x\n","  x+=1\n","  "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"x9bsmsiMG7rL","executionInfo":{"status":"ok","timestamp":1602259189731,"user_tz":-330,"elapsed":1358,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Setting index of <PAD> token as 0 \n","vocab['cameron']= vocab['<PAD>']\n","vocab['<PAD>']=0"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9Greh6zHQ5w","executionInfo":{"status":"ok","timestamp":1602259189731,"user_tz":-330,"elapsed":1142,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["# Inversing the dictionary\n","inv_vocab = {w:v for v,w in vocab.items()}"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nnym9xy21yy4"},"source":["### <b>Creating Encoder and Decoder Input for Training </b>\n","The encoder-decoder model is a way of using recurrent neural networks for sequence-to-sequence prediction problems.<br>\n","The overall most basic structure of sequence to sequence model(encoder-decoder) which is commonly used is as shown below-\n","![alt text](https://miro.medium.com/max/1250/1*zq1G3mPSuy-KoMlBldohww.png)\n","It consists of 3 parts:<b> encoder, intermediate vector</b> and <b>decoder.</b><br>\n","<br>\n","<b>Encoder-</b>It accepts a single element of the input sequence at each time step, process it, collects information for that element and propagates it forward.<br>\n","<b>Intermediate vector-</b> This is the final internal state produced from the encoder part of the model. It contains information about the entire input sequence to help the decoder make accurate predictions.<br>\n","<b>Decoder-</b> given the entire sentence, it predicts an output at each time step."]},{"cell_type":"code","metadata":{"id":"huOVgCL0H4Yb","executionInfo":{"status":"ok","timestamp":1602259190247,"user_tz":-330,"elapsed":1147,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"eb8bb1f0-bc93-4e5a-c838-f649e81897c0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["### Creating encoder input to feed the model\n","encoder_inp = []\n","for line in clean_ques:\n","  lst=[]\n","  for word in line.split():\n","    if word not in vocab:\n","      lst.append(vocab['<OUT>'])\n","    else:\n","      lst.append(vocab[word])\n","  encoder_inp.append(lst)\n","encoder_inp[:10]"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[3467], [1], [2], [3, 4], [5], [6, 7], [8, 9], [10], [11, 12], [9]]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"aycAEaSHL64S","executionInfo":{"status":"ok","timestamp":1602259190830,"user_tz":-330,"elapsed":1459,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"2a58e6cf-141a-4d49-d26a-14a7a455da10","colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["### Creating decoder input for answers\n","decoder_inp = []\n","for line in clean_ans:\n","  lst=[]\n","  for word in line.split():\n","    if word not in vocab:\n","      lst.append(vocab['<OUT>'])\n","    else:\n","      lst.append(vocab[word])\n","  decoder_inp.append(lst)\n","\n","decoder_inp[5:10]"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[3470, 47, 33, 1175, 18, 31, 35, 209, 50, 111, 47, 3468],\n"," [3470, 85, 27, 28, 147, 47, 783, 18, 74, 3468],\n"," [3470, 47, 835, 1300, 46, 1638, 3468],\n"," [3470, 50, 46, 3469, 1581, 3469, 6, 40, 370, 1462, 516, 3468],\n"," [3470, 15, 47, 83, 2225, 1377, 18, 1943, 60, 18, 804, 3468]]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"2LJM5O2e4u1_"},"source":["#### <b>Padding</b>\n","What does this mean? It means that we need to search for the length of the longest sentence, convert every sentence to a vector of that length, and fill the gap between the number of words of each sentence, and the number of words of the longest sentences with zeros.<br>\n","Why Padding?We need padding because our model expects all\n","the sentences in a batch whether they are questions or answers must have the same length."]},{"cell_type":"code","metadata":{"id":"_w0adWJQM09E","executionInfo":{"status":"ok","timestamp":1602259192858,"user_tz":-330,"elapsed":2980,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["from keras.preprocessing.sequence import pad_sequences\n","### Padding our encoder and decoder values\n","encoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\n","decoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2pT1AkVOLCI","executionInfo":{"status":"ok","timestamp":1602259192864,"user_tz":-330,"elapsed":2665,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"3acf2abd-bc0a-4111-cb4a-e39737af871f","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["encoder_inp[:5]"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3467,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [   2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [   3,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [   5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Nl0zj8EOQ7gL","executionInfo":{"status":"ok","timestamp":1602259192867,"user_tz":-330,"elapsed":2442,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"d44e25b7-a0dd-4255-f502-c0b61cd3bd34","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["decoder_inp[:5]"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3470,   61,  538,   28, 3467,    6,   40,  285,   61, 3469,   69,\n","          88, 2405],\n","       [3470, 3469,  967,   14, 1077,   18,  230,  126, 2406,   71,   14,\n","        2407,  647],\n","       [3470,  152, 3468,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [3470,    6,  126,  126,  126, 1069,   31,   10,    6,  238,   29,\n","          29, 1413],\n","       [3470, 1551,  370,  911, 1018,   35,  106,   65, 3468,    0,    0,\n","           0,    0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"iewe43ok6EZs"},"source":["We created this new list of decoder as there is always a same token(3470) at the begining of every list, this is the token of `<SOS>` string, we need to remove this from our decoder model, otherwise the predicting model will always be predicting this at the start of every replied string."]},{"cell_type":"code","metadata":{"id":"tBsNGv_vOwGC","executionInfo":{"status":"ok","timestamp":1602259192870,"user_tz":-330,"elapsed":1096,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"b5ff09dc-9acd-43ef-95b1-4c620e0ff955","colab":{"base_uri":"https://localhost:8080/","height":185}},"source":["final_decoder = []\n","for inp in decoder_inp:\n","  final_decoder.append(inp[1:])\n","final_decoder = pad_sequences(final_decoder, 13, padding='post', truncating='post')\n","final_decoder[:5]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  61,  538,   28, 3467,    6,   40,  285,   61, 3469,   69,   88,\n","        2405,    0],\n","       [3469,  967,   14, 1077,   18,  230,  126, 2406,   71,   14, 2407,\n","         647,    0],\n","       [ 152, 3468,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0],\n","       [   6,  126,  126,  126, 1069,   31,   10,    6,  238,   29,   29,\n","        1413,    0],\n","       [1551,  370,  911, 1018,   35,  106,   65, 3468,    0,    0,    0,\n","           0,    0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"ULGTm2HFR5jM","executionInfo":{"status":"ok","timestamp":1602259193398,"user_tz":-330,"elapsed":974,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"c84233ba-add3-48ef-9be2-d0717789d969","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.shape(final_decoder)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31416, 13)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"CfmuRXpO61_5"},"source":["<b> Reshaping Decoder Input</b><br>\n","We need to convert our final_decoder data to 3D data, as our model expects 3D data for training.\n","Here, we will we using `to_categorical()` function for this purpose, it converts a class vector (integers) to binary class matrix."]},{"cell_type":"code","metadata":{"id":"nfrUHfkOxRTm","executionInfo":{"status":"ok","timestamp":1602259197988,"user_tz":-330,"elapsed":3862,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"9b665a6c-6e53-447f-9d57-4a39a779ec8d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from tensorflow.keras.utils import to_categorical\n","final_decoder = to_categorical(final_decoder, len(vocab))\n","final_decoder.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31416, 13, 3471)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"ZUJhupW071TX"},"source":["## <b>Create Model</b>\n","Here we will be using Recurrent Neural Networks.The RNN used here is <b>Long Short Term Memory(LSTM).</b> \n","<br>\n","Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.\n","<br>\n","LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior.\n","\n","The Encoder loops over the sequence. At each step it takes the encoded representation of a token and passes it through the LSTM cell, that also gets an input of the state of the sentence representation so far. When EOS is reached, we collect the final state of our cell (cell_state, hidden_state).\n","![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/encoder.png)\n","<br>\n","<br>\n","The Decoder shares the same basic architecture, with one added layer (here a perceptron layer) to predict the new token.\n","![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/decoder.png)"]},{"cell_type":"code","metadata":{"id":"-CikiQP8Qxaz","executionInfo":{"status":"ok","timestamp":1602259202337,"user_tz":-330,"elapsed":1476,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"hymDzTcskH2F","executionInfo":{"status":"ok","timestamp":1602259202339,"user_tz":-330,"elapsed":910,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating input placeholders for our encoder and decoder\n","enc_inp = Input(shape=(13,))\n","dec_inp = Input(shape=(13,))\n"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"udn0wjDA_p7M"},"source":["#### <b>Word Embedding</b>\n","Word embeddings are mathematical representations of words encoded as vectors in n-dimentional space. Similar words are close to each other in this space. This means that we can compare 2 or more words to each other not by e.g. the number of overlapping characters but by how close they are to each other in they embedded form.\n","<br>\n","<b>Why Word Embedding?</b><br>\n","Our goal in designing an intent detection is to create a system that, given a few examples for intent, can detect that a sentence given by the user is similar to these examples and therefore should have the same intent.\n","The problem behind this system is that we have to design a system for checking if 2 sentences are similar. This could be achieved by eg. counting how many overlapping words are in the new sentence and the sentences in training data set. This is however a naive approach because a user can use a word that has similar meaning, but is different from the ones in the train examples.\n"]},{"cell_type":"code","metadata":{"id":"9idpZo4Tkf3D","executionInfo":{"status":"ok","timestamp":1602259202776,"user_tz":-330,"elapsed":847,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating embedding layer\n","Vocab_Size=len(vocab)\n","embed = Embedding(Vocab_Size+1, output_dim=70, input_length=13, trainable=True)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQSsugW1lgfQ","executionInfo":{"status":"ok","timestamp":1602259209289,"user_tz":-330,"elapsed":7118,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Embedding encoder input\n","enc_embed = embed(enc_inp)\n","### Creating object for encoder LSTM Layer\n","enc_lstm = LSTM(700, return_sequences=True, return_state=True)\n","enc_op, h,c = enc_lstm(enc_embed)\n","enc_states=[h,c]\n"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"5uPN9inLm6mU","executionInfo":{"status":"ok","timestamp":1602259209293,"user_tz":-330,"elapsed":6629,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Embedding decoder input\n","dec_embed = embed(dec_inp)\n","### Creating object for decoder LSTM Layer\n","dec_lstm = LSTM(700, return_sequences=True, return_state=True)\n","dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5eLNHWyCivO"},"source":["#### <b>Dense Layer</b>\n","A Dense Layer is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. The layer has a weight matrix <b>W</b>, a bias vector <b>b</b>, and the activations of previous layer <b>a</b>. "]},{"cell_type":"code","metadata":{"id":"F-fXQbqpnml4","executionInfo":{"status":"ok","timestamp":1602259212062,"user_tz":-330,"elapsed":1347,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating dense layer\n","dense = Dense(Vocab_Size, activation='softmax')\n","dense_output = dense(dec_op)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"hRODQ25Xn_19","executionInfo":{"status":"ok","timestamp":1602259215126,"user_tz":-330,"elapsed":1263,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating and Compiling Model \n","model = Model([enc_inp, dec_inp], dense_output)\n","model.compile(loss='categorical_crossentropy',metrics=['acc'], optimizer='adam')"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"nW2E25ZJrI-J","executionInfo":{"status":"ok","timestamp":1602259215129,"user_tz":-330,"elapsed":784,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"ce0e9bde-ed22-46bd-d5a0-0abd788985c7","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Vocab_Size"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3471"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"EyP7FI1Noif3","executionInfo":{"status":"ok","timestamp":1602260458389,"user_tz":-330,"elapsed":847126,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"bb520089-df6f-48ae-b47e-3d4673434c3c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["### Fitting the data to our model\n","model.fit([encoder_inp, decoder_inp], final_decoder, epochs=60)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Epoch 1/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.7836 - acc: 0.5191\n","Epoch 2/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.6620 - acc: 0.5264\n","Epoch 3/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.5754 - acc: 0.5313\n","Epoch 4/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.4979 - acc: 0.5356\n","Epoch 5/60\n","982/982 [==============================] - 20s 20ms/step - loss: 2.4183 - acc: 0.5393\n","Epoch 6/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.3325 - acc: 0.5439\n","Epoch 7/60\n","982/982 [==============================] - 20s 20ms/step - loss: 2.2390 - acc: 0.5497\n","Epoch 8/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.1391 - acc: 0.5584\n","Epoch 9/60\n","982/982 [==============================] - 19s 20ms/step - loss: 2.0353 - acc: 0.5703\n","Epoch 10/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.9344 - acc: 0.5855\n","Epoch 11/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.8377 - acc: 0.6015\n","Epoch 12/60\n","982/982 [==============================] - 19s 20ms/step - loss: 1.7490 - acc: 0.6187\n","Epoch 13/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.6647 - acc: 0.6350\n","Epoch 14/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.5852 - acc: 0.6524\n","Epoch 15/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.5122 - acc: 0.6683\n","Epoch 16/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.4446 - acc: 0.6845\n","Epoch 17/60\n","982/982 [==============================] - 19s 20ms/step - loss: 1.3813 - acc: 0.6991\n","Epoch 18/60\n","982/982 [==============================] - 20s 21ms/step - loss: 1.3235 - acc: 0.7128\n","Epoch 19/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.2711 - acc: 0.7254\n","Epoch 20/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.2219 - acc: 0.7368\n","Epoch 21/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.1788 - acc: 0.7468\n","Epoch 22/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.1383 - acc: 0.7562\n","Epoch 23/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.1015 - acc: 0.7647\n","Epoch 24/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.0691 - acc: 0.7720\n","Epoch 25/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.0383 - acc: 0.7781\n","Epoch 26/60\n","982/982 [==============================] - 20s 20ms/step - loss: 1.0109 - acc: 0.7836\n","Epoch 27/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.9853 - acc: 0.7893\n","Epoch 28/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.9614 - acc: 0.7936\n","Epoch 29/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.9402 - acc: 0.7976\n","Epoch 30/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.9183 - acc: 0.8021\n","Epoch 31/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8993 - acc: 0.8056\n","Epoch 32/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8814 - acc: 0.8082\n","Epoch 33/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8634 - acc: 0.8122\n","Epoch 34/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8477 - acc: 0.8143\n","Epoch 35/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8322 - acc: 0.8171\n","Epoch 36/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8152 - acc: 0.8203\n","Epoch 37/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.8012 - acc: 0.8228\n","Epoch 38/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7871 - acc: 0.8257\n","Epoch 39/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7749 - acc: 0.8277\n","Epoch 40/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7625 - acc: 0.8296\n","Epoch 41/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7462 - acc: 0.8332\n","Epoch 42/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7342 - acc: 0.8352\n","Epoch 43/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7249 - acc: 0.8369\n","Epoch 44/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7112 - acc: 0.8393\n","Epoch 45/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.7005 - acc: 0.8416\n","Epoch 46/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6901 - acc: 0.8434\n","Epoch 47/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6802 - acc: 0.8449\n","Epoch 48/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6688 - acc: 0.8479\n","Epoch 49/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6586 - acc: 0.8497\n","Epoch 50/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6469 - acc: 0.8521\n","Epoch 51/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6369 - acc: 0.8546\n","Epoch 52/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6351 - acc: 0.8539\n","Epoch 53/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6247 - acc: 0.8560\n","Epoch 54/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6100 - acc: 0.8600\n","Epoch 55/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.6051 - acc: 0.8605\n","Epoch 56/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.5995 - acc: 0.8613\n","Epoch 57/60\n","982/982 [==============================] - 20s 20ms/step - loss: 0.5915 - acc: 0.8627\n","Epoch 58/60\n","982/982 [==============================] - 20s 21ms/step - loss: 0.5809 - acc: 0.8655\n","Epoch 59/60\n","982/982 [==============================] - 20s 21ms/step - loss: 0.5754 - acc: 0.8665\n","Epoch 60/60\n","982/982 [==============================] - 20s 21ms/step - loss: 0.5710 - acc: 0.8674\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fe502602dd8>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"yRo-OBgV1Jmu","executionInfo":{"status":"ok","timestamp":1602260489105,"user_tz":-330,"elapsed":8866,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"25623849-3d1f-4ea3-b329-aa1357cc7fdf","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","# Save the entire model as a SavedModel.\n","!mkdir -p saved_model\n","model.save('saved_model/my_model') "],"execution_count":40,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZxC_rIjG1uzL"},"source":["# # my_model directory\n","# !ls saved_model\n","\n","# # Contains an assets folder, saved_model.pb, and variables folder.\n","# !ls saved_model/my_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6It08ST11Yn"},"source":["# new_model = tf.keras.models.load_model('saved_model/my_model')\n","\n","# # Check its architecture\n","# new_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cViFcgFxFC3f"},"source":["##<b> Inference</b>"]},{"cell_type":"code","metadata":{"id":"rTz4MIUepU2T","executionInfo":{"status":"ok","timestamp":1602260529603,"user_tz":-330,"elapsed":1448,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating encoder model \n","enc_model = Model([enc_inp], enc_states)"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"kh1svEQmgtkb","executionInfo":{"status":"ok","timestamp":1602260529604,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}}},"source":["### Creating decoder model\n","decoder_state_input_h = Input(shape=(700,))\n","decoder_state_input_c = Input(shape=(700,))\n","### Creating list of decoder h,c\n","decoder_state_input = [decoder_state_input_h, decoder_state_input_c]"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K3xrOILnDqu4"},"source":["#### <b>How decoder creates new sentences?</b>\n","The encoder provides a representation of the whole sequence. One option is to initialize the decoder state with this representation, then just send an SOS token to start the generation of the new sequence.<br>\n","In order to provide to the decoder a maximum of information about its progression, we can pass the decoded token as the new input when generating the next one. We repeat the process until the decoder raises the EOS signal.<br>\n","![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/Seq2Seq_inference-1-1920x453.png)\n"]},{"cell_type":"code","metadata":{"id":"8hPQEOe0hpkK","executionInfo":{"status":"ok","timestamp":1602057206266,"user_tz":-330,"elapsed":324468,"user":{"displayName":"Md Ajmal Shadab","photoUrl":"","userId":"16662657698690775435"}},"outputId":"0c8abb6b-ce07-4183-d6a8-e75ab35d2ed4","colab":{"base_uri":"https://localhost:8080/"}},"source":["decoder_outputs, state_h, state_c = dec_lstm(dec_embed, initial_state=decoder_state_input)\n","decoder_states = [state_h, state_c]\n","decoder_model = Model([dec_inp]+decoder_state_input, [decoder_outputs]+decoder_states)\n","print(\"#######################################################################\")\n","print(\"#                       Starting Chatbot ver 1.0                      #\")\n","print(\"#######################################################################\")\n","\n","### Creating user input condition\n","prepro1= \"\"\n","while prepro1 != 'q':\n","  prepro1 = input(\"You: \")\n","  # Cleaning the text\n","  prepro1 = clean_text(prepro1)\n","  prepro = [prepro1]\n","\n","  # Creating a list for processed text\n","  new_txt = []\n","  for x in prepro:\n","    new_list = []\n","    for y in x.split():\n","      try:\n","        new_list.append(vocab[y])\n","      except:\n","        new_list.append(vocab['<OUT>'])\n","    new_txt.append(new_list)\n","\n","  # Applying pad to our new_txt list\n","  new_txt = pad_sequences(new_txt, 13, padding='post')\n","  # Predicting the txt input\n","  stat = enc_model.predict(new_txt)\n","\n","  empty_target_seq = np.zeros( (1, 1) )\n","  empty_target_seq[0,0] = vocab['<SOS>']\n","\n","  stop_condition = False\n","  decoded_translation = ''\n","\n","  while not stop_condition:\n","    decoder_outputs,h,c = decoder_model.predict([empty_target_seq]+stat)\n","    decoder_concat_inp = dense(decoder_outputs)\n","\n","    sampled_word_index = np.argmax(decoder_concat_inp[0, -1, :])\n","    sampled_word = inv_vocab[sampled_word_index]+ ' '\n","\n","    if sampled_word!= '<EOS> ':\n","      decoded_translation += sampled_word\n","    \n","    if sampled_word == '<EOS> ' or len(decoded_translation.split())>13:\n","      stop_condition=True\n","  \n","    ## Reseting the variabls\n","    empty_target_seq = np.zeros( (1, 1) ) \n","    empty_target_seq[0,0] = sampled_word_index\n","    stat = [h,c]\n","  \n","  print(\"Chatbot Attention: \", decoded_translation)\n","  print(\"==============================================================\")\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["#######################################################################\n","#                       Starting Chatbot ver 1.0                      #\n","#######################################################################\n","You: hii\n","WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_10:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n","Chatbot Attention:  <OUT> \n","==============================================================\n","You: hi\n","Chatbot Attention:  hi \n","==============================================================\n","You: What are you doing\n","Chatbot Attention:  do you have a guy like me every year a guy <PAD> <PAD> <PAD> \n","==============================================================\n","You: No\n","Chatbot Attention:  you are sure \n","==============================================================\n","You: Yes\n","Chatbot Attention:  i am glad you came i am sorry about the <OUT> \n","==============================================================\n","You: Yes\n","Chatbot Attention:  i am glad you came i am sorry about the <OUT> \n","==============================================================\n","You: You are a bot\n","Chatbot Attention:  i am sure \n","==============================================================\n","You: Are you a girl\n","Chatbot Attention:  what do you mean julie \n","==============================================================\n","You: Your name is julie\n","Chatbot Attention:  <OUT> <OUT> <OUT> you could search more sky there in a day <PAD> <PAD> \n","==============================================================\n","You: Are you a boy\n","Chatbot Attention:  we are gonna get mom back all here buzz hold us \n","==============================================================\n","You: do you know me\n","Chatbot Attention:  no i am going home \n","==============================================================\n","You: where is your home\n","Chatbot Attention:  <OUT> guns down <OUT> up \n","==============================================================\n","You: what about guns\n","Chatbot Attention:  the <OUT> <OUT> the <OUT> into the flesh of the man very <PAD> <PAD> \n","==============================================================\n","You: are you alright\n","Chatbot Attention:  yeah \n","==============================================================\n","You: are you sure about that\n","Chatbot Attention:  what do you mean kinda yeah what do you think of it <PAD> <PAD> \n","==============================================================\n","You: yeah its ok my bad\n","Chatbot Attention:  you are a good man \n","==============================================================\n","You: thanks\n","Chatbot Attention:  i mean i would hate to think i would pick up someone <PAD> <PAD> \n","==============================================================\n","You: why \n","Chatbot Attention:  because you have to leave half an <OUT> at the tip to <PAD> <PAD> \n","==============================================================\n","You: to whom\n","Chatbot Attention:  to a girl here \n","==============================================================\n","You: where is she\n","Chatbot Attention:  i dont know \n","==============================================================\n","You: who is that girl\n","Chatbot Attention:  <OUT> <OUT> <OUT> north <OUT> nuclear <OUT> \n","==============================================================\n","You: you are funny\n","Chatbot Attention:  i am sure \n","==============================================================\n","You: yeah\n","Chatbot Attention:  i am sorry but i can not understand i can not have <PAD> <PAD> \n","==============================================================\n","You: ok thanks for chatting\n","Chatbot Attention:  i will do my best madame even if i have to work <PAD> <PAD> \n","==============================================================\n","You: i am not madame\n","Chatbot Attention:  you are not taking real good care of yourself \n","==============================================================\n","You: i knw\n","Chatbot Attention:  you are a <OUT> \n","==============================================================\n","You: what you want to say\n","Chatbot Attention:  i dont know but close \n","==============================================================\n","You: ok i am going\n","Chatbot Attention:  i am serious get this i had a careful look at that <PAD> <PAD> \n","==============================================================\n","You: bye\n","Chatbot Attention:  bye \n","==============================================================\n","You: q\n","Chatbot Attention:  <OUT> \n","==============================================================\n"],"name":"stdout"}]}]}