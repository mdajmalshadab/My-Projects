{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat-Bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nswQpvw6cM-9"
      },
      "source": [
        "# <b>CHAT-BOT- Seq2Seq Model</b>\n",
        "A chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent. Designed to convincingly simulate the way a human would behave as a conversational partner, chatbot systems typically require continuous tuning and testing, and many in production remain unable to adequately converse or pass the industry standard Turing test. The term \"ChatterBot\" was originally coined by Michael Mauldin (creator of the first Verbot) in 1994 to describe these conversational programs. \n",
        "![alt text](https://static.vecteezy.com/system/resources/previews/000/343/481/non_2x/chatbot-write-answer-to-messages-in-the-chat-bot-consultant-is-free-to-help-users-in-your-phone-online-vector-cartoon-illustration.jpg)\n",
        "\n",
        "The basic foundation of chatbots is providing the best response of any query that it receives. The best response like answering the sender questions, providing sender relevant information, ask follow-up questions and do the conversation in realistic way.\n",
        "The chatbot needs to be able to understand the intentions of the sender’s message, determine what type of response message (a follow-up question, direct response, etc.) is required, and follow correct grammatical and lexical rules while forming the response.<br>\n",
        "<br>\n",
        "<b>What is Seq2Seq Model?</b><br>\n",
        "Seq2Seq is a machine learning architecture based on the encoder-decoder paradigm. It is widely used for tasks such as translation, Q&A and other cases where it is desirable to produce a sequence from another. The main idea is to have one model, for example an RNN, which can create a good representation of the input sequence. We will refer to this model as the ‘encoder’. Using this representation, another model, the ‘decoder’, produces the expected output sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBy4dcIVhox2"
      },
      "source": [
        "## <b>About the Data</b>\n",
        "### <b>Cornell Movie--Dialogs Corpus</b>\n",
        "#### <b>Description</b>\n",
        "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:\n",
        "\n",
        "- 220,579 conversational exchanges between 10,292 pairs of movie characters\n",
        "\n",
        "- involves 9,035 characters from 617 movies\n",
        "\n",
        "- in total 304,713 utterances\n",
        "In all files the field separator is \" +++$+++ \".<br>\n",
        "\n",
        "#### <b>movie_lines.txt</b>\n",
        "- contains the actual text of each utterance\n",
        "- fields:\n",
        "  - lineID\n",
        "  - characterID (who uttered this phrase)\n",
        "  - movieID\n",
        "  - character name\n",
        "  - text of the utterance\n",
        "\n",
        "#### <b>movie_conversations.txt</b>\n",
        "- the structure of the conversations\n",
        "- fields\n",
        "  - characterID of the first character involved in the conversation\n",
        "  - characterID of the second character involved in the conversation\n",
        "  - movieID of the movie in which the conversation occurred\n",
        "  - list of the utterances that make the conversation, in chronological order: ['lineID1','lineID2',...,'lineIDN']\n",
        "has to be matched with movie_lines.txt to reconstruct the actual content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wrzCHZTzj2c"
      },
      "source": [
        "### Importing some libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41CU_8E0pSfn",
        "outputId": "fc605dd1-9e1e-4908-f13f-37ebfb0ac4f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "### Reading the data as list\n",
        "lines = open('/content/drive/My Drive/Chat-Bot Project/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "lines[:10]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\",\n",
              " 'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow',\n",
              " \"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\",\n",
              " 'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No',\n",
              " 'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?',\n",
              " 'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YPXpXCqrrW8",
        "outputId": "6c3465e2-b916-4669-9191-48ec8502e9ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "conversation = open('/content/drive/My Drive/Chat-Bot Project/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conversation[:10]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366']\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PvaZcvkQlR"
      },
      "source": [
        "### <b>Data Preprocessing</b>\n",
        "In the data preprocessing process we would first filter the data out from unnecessary ids and separators.\n",
        "<br>\n",
        "<b>Step 1: Data Retrieving</b><br>\n",
        "Here we will create a list of dialogue exchange id list all over from the conversation list.\n",
        "<br>\n",
        "After retrieving the dialogue exchange id list from the conversation list, I will create a dictionary to map the conversation lines with their respective ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Key1PTvgsN0T",
        "outputId": "9f406139-ee04-46ed-b5e3-50b517d466c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "exchange_list= []\n",
        "for conver in conversation:\n",
        "  exchange_list.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\",\",\"\").split())\n",
        "exchange_list[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['L194', 'L195', 'L196', 'L197'],\n",
              " ['L198', 'L199'],\n",
              " ['L200', 'L201', 'L202', 'L203'],\n",
              " ['L204', 'L205', 'L206'],\n",
              " ['L207', 'L208'],\n",
              " ['L271', 'L272', 'L273', 'L274', 'L275'],\n",
              " ['L276', 'L277'],\n",
              " ['L280', 'L281'],\n",
              " ['L363', 'L364'],\n",
              " ['L365', 'L366']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IafNxfXGtgN5",
        "outputId": "ff49ab40-3abe-4a8c-db14-6e58f275bcdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "### Creating a dictionary with key as id and dialogues as their values\n",
        "dialogues = {}\n",
        "for line in lines:\n",
        "  dialogues[line.split(' +++$+++ ')[0]]= line.split(' +++$+++ ')[-1]\n",
        "\n",
        "val_cnt=0\n",
        "for ind in dialogues:\n",
        "  print('{}: \"{}\"'.format(ind,dialogues[ind]))\n",
        "  val_cnt+=1\n",
        "  if val_cnt==20:\n",
        "    break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1045: \"They do not!\"\n",
            "L1044: \"They do to!\"\n",
            "L985: \"I hope so.\"\n",
            "L984: \"She okay?\"\n",
            "L925: \"Let's go.\"\n",
            "L924: \"Wow\"\n",
            "L872: \"Okay -- you're gonna need to learn how to lie.\"\n",
            "L871: \"No\"\n",
            "L870: \"I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\"\n",
            "L869: \"Like my fear of wearing pastels?\"\n",
            "L868: \"The \"real you\".\"\n",
            "L867: \"What good stuff?\"\n",
            "L866: \"I figured you'd get to the good stuff eventually.\"\n",
            "L865: \"Thank God!  If I had to hear one more story about your coiffure...\"\n",
            "L864: \"Me.  This endless ...blonde babble. I'm like, boring myself.\"\n",
            "L863: \"What crap?\"\n",
            "L862: \"do you listen to this crap?\"\n",
            "L861: \"No...\"\n",
            "L860: \"Then Guillermo says, \"If you go any lighter, you're gonna look like an extra on 90210.\"\"\n",
            "L699: \"You always been this selfish?\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9MAkbUKmtn3"
      },
      "source": [
        "<b>Step 2: Data Reshaping</b><br>\n",
        "In this process we need to create two separate lists, one for questions and other for answers. The `questions` and `answers` list formation can be explained as follows:<br>\n",
        "The first dialogue of a particular conversation will act as the question for another person, and the next dialogue/reply of another person will be the answer for the first dialogue as well as it will act as a question for upcoming dialogue by any other person or maybe the first person. This is how a conversation between two or more people works. This cycle goes on until the conversation ends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ7GNKXwvJRL",
        "outputId": "d5832146-5343-484e-c9af-d54f1d14ab4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "### Creating list of questions and answers\n",
        "questions = []\n",
        "answers = []\n",
        "for conver in exchange_list:\n",
        "  for i in range(len(conver)-1):\n",
        "      questions.append(dialogues[conver[i]])\n",
        "      answers.append(dialogues[conver[i+1]])\n",
        "\n",
        "print(np.shape(questions))\n",
        "print(np.shape(answers))\n",
        "questions[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(221616,)\n",
            "(221616,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              " \"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'Not the hacking and gagging and spitting part.  Please.',\n",
              " \"You're asking me out.  That's so cute. What's your name again?\",\n",
              " \"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
              " 'Cameron.',\n",
              " \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              " 'Why?',\n",
              " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              " 'Gosh, if only we could find Kat a boyfriend...']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPRybko3zIgj",
        "outputId": "6b1a2224-f9cd-4fbb-b07e-0b9cf025fdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "answers[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              " 'Not the hacking and gagging and spitting part.  Please.',\n",
              " \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
              " 'Forget it.',\n",
              " 'Cameron.',\n",
              " \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              " 'Seems like she could get a date easy enough...',\n",
              " 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              " \"That's a shame.\",\n",
              " 'Let me see what I can do.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arCLW3bno-Rc"
      },
      "source": [
        "<b>Step 3: Cutting down data size</b><br>\n",
        "As constrained to the computation limitatons, we need to chop down the data on the basis of length of the question. We are going to consider only those questions whose length is below 13. I know it is pretty short, but I dont have enough RAM to compute such a huge amount of data all at once.<br>\n",
        "So further from here we will be using only those conversation(`sorted_ques` and `sorted_ans`)whose length of the question is below 13.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZi0wkt0D8y",
        "outputId": "cd5e52f2-4d81-4169-9f8e-273a5cf69d7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "### Creating a list of sorted questions and answers(questions wih length less than 13)\n",
        "sorted_ques=[]\n",
        "sorted_ans=[]\n",
        "for i in range(len(questions)):\n",
        "  if len(questions[i])<13:\n",
        "    sorted_ques.append(questions[i])\n",
        "    sorted_ans.append(answers[i])\n",
        "print(\"Size of Sorted questions: {}\".format(np.size(sorted_ques)))\n",
        "print(\"Size of Sorted answers: {}\".format(np.size(sorted_ans)))\n",
        "sorted_ques[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Sorted questions: 31416\n",
            "Size of Sorted answers: 31416\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cameron.',\n",
              " 'Why?',\n",
              " 'There.',\n",
              " 'Sure have.',\n",
              " 'Hi.',\n",
              " 'I was?',\n",
              " 'Well, no...',\n",
              " 'But',\n",
              " 'What crap?',\n",
              " 'No']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p9R_wmYrgMG"
      },
      "source": [
        "<b>Step 4: Data Cleaning</b><br>\n",
        "A regular expression, often abbreviated to regex, is a method of using a sequence of characters to define a search to match strings, i.e. “find and replace”-like operations.<br>\n",
        "This step is required because there are many words used in english language which is used as their short-form for example:<br>\n",
        "He's for He is<br>\n",
        "'d for would<br>\n",
        "'ll for will<br>...etc.<br>\n",
        "\n",
        "So here we will be changing all such words into their respective standard words using the <b>\"Regular Expression(re)\"</b> Library.\n",
        "we will be also converting all the uppercase letters into lowercase, then we will remove all the puncuation marks from our data.<br>\n",
        "If we don't change all these words, our model will interpret them as two different words, which is not desirable. Puncuation marks are not required because our bot won't understand them.<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZPXsT3637yq"
      },
      "source": [
        "# Importing Regular Expression Library(re)\n",
        "import re\n",
        "# Creating a function to clean the text (removing punctuation marks)\n",
        "def clean_text(txt):\n",
        "  txt = txt.lower()\n",
        "  txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "  txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "  txt = re.sub(r\"she's\", \"she is\",txt)\n",
        "  txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "  txt = re.sub(r\"what's\", \"what is\", txt)\n",
        "  txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "  txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "  txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "  txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "  txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "  txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "  txt = re.sub(r\"can't\", \"can not\", txt)\n",
        "  txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "  return txt\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5B9uStn7FfT",
        "outputId": "67208120-5701-465f-bb24-d18fa744371b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Creating a list of clean questions and clean answers\n",
        "clean_ques=[]\n",
        "clean_ans=[]\n",
        "for line in sorted_ques:\n",
        "  clean_ques.append(clean_text(line))\n",
        "for line in sorted_ans:\n",
        "  clean_ans.append(clean_text(line))\n",
        "clean_ques[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cameron',\n",
              " 'why',\n",
              " 'there',\n",
              " 'sure have',\n",
              " 'hi',\n",
              " 'i was',\n",
              " 'well no',\n",
              " 'but',\n",
              " 'what crap',\n",
              " 'no']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R56R0-sDvwhR"
      },
      "source": [
        "Along with the questions we will also cut down the length for the answers for the same reason, and faster computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1u2EMDH7_6-",
        "outputId": "51d1a045-617e-4cd9-d567-ffc3cdbf5b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Cutting down long size answers\n",
        "for i in range(len(clean_ans)):\n",
        "  clean_ans[i]= ' '.join(clean_ans[i].split()[:15])\n",
        "clean_ans[:20]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the thing is cameron i am at the mercy of a particularly hideous breed of',\n",
              " 'unsolved mystery she used to be really popular when she started high school then it',\n",
              " 'where',\n",
              " 'i really really really wanna go but i can not not unless my sister goes',\n",
              " 'looks like things worked out tonight huh',\n",
              " 'you never wanted to go out with me did you',\n",
              " 'then that is all you had to say',\n",
              " 'you always been this selfish',\n",
              " 'me this endless blonde babble i am like boring myself',\n",
              " 'okay you are gonna need to learn how to lie',\n",
              " 'lets go',\n",
              " 'i hope so',\n",
              " 'they do not',\n",
              " 'you might wanna think about it',\n",
              " 'joey',\n",
              " 'would you mind getting me a drink cameron',\n",
              " 'expensive',\n",
              " 'its a gay cruise line but i will be like wearing a uniform and stuff',\n",
              " 'my agent says i have got a good shot at being the prada guy next',\n",
              " 'you are concentrating awfully hard considering its gym class']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGD82wHawL7g"
      },
      "source": [
        "There are several words in our dialogue list(`clean_ques` and `clean_ans`) which does not appear very frequently, like names of some place, person or thing. We need to remove all those words whose length is lesser tham=n some threshold value, because it won't be helping our model to generalise efficiently.<br>\n",
        "Here we can see that there are 17713 different words available in our cleaned dialogues list, and it reduced to 3467 after removing the less repeating words(here it is less than 5 times)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCfnDNw9KXR",
        "outputId": "32c1ca91-7722-4de5-f305-567c2a16005c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Creating a word count dictionary for the counts of different words\n",
        "word_count ={}\n",
        "for line in clean_ques:\n",
        "  for word in line.split():\n",
        "    if word not in word_count:\n",
        "      word_count[word]=1\n",
        "    else:\n",
        "      word_count[word]+=1\n",
        "\n",
        "for line in clean_ans:\n",
        "  for word in line.split():\n",
        "    if word not in word_count:\n",
        "      word_count[word]=1\n",
        "    else:\n",
        "      word_count[word]+=1\n",
        "\n",
        "print(len(word_count))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bh4bp-4l_pux",
        "outputId": "14d684d5-3a09-4489-fc74-76ca93efb6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Removing words whose count is below threshold value\n",
        "threshold= 5\n",
        "# Creating a new word wise dictionary/vocabulary to store filtered data\n",
        "vocab = {}\n",
        "cnt =0;\n",
        "for word,count in word_count.items():\n",
        "  if count>=threshold:\n",
        "    vocab[word]=cnt\n",
        "    cnt+=1\n",
        "print(len(vocab))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0sIt_ZPzq7B"
      },
      "source": [
        "<b> Step 4: Solving the possible variation between the input and output sequence sizes</b><br>\n",
        "One issue to be confronted when choosing a seq2seq approach is the possible variation between the input and output sequence sizes. To handle that, we can introduce the SOS (Start Of Sequence) and EOS (End Of Sequence) tokens. By adding the EOS token at the end of each input we provide a consistent signal, facilitating the system’s capacity of learning how to finish the creation of the new sequence. We can then ask the decoder to give as many tokens as it wants until it raises the EOS token to signal the end of the output sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9PoalzsBaqP",
        "outputId": "8c81ac84-b810-474d-90d3-094acc6208ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Adding <SOS> and <EOS> at begining and end of string\n",
        "for i in range(len(clean_ans)):\n",
        "  clean_ans[i]= '<SOS> ' + clean_ans[i] + ' <EOS>'\n",
        "clean_ans[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<SOS> the thing is cameron i am at the mercy of a particularly hideous breed of <EOS>',\n",
              " '<SOS> unsolved mystery she used to be really popular when she started high school then it <EOS>',\n",
              " '<SOS> where <EOS>',\n",
              " '<SOS> i really really really wanna go but i can not not unless my sister goes <EOS>',\n",
              " '<SOS> looks like things worked out tonight huh <EOS>',\n",
              " '<SOS> you never wanted to go out with me did you <EOS>',\n",
              " '<SOS> then that is all you had to say <EOS>',\n",
              " '<SOS> you always been this selfish <EOS>',\n",
              " '<SOS> me this endless blonde babble i am like boring myself <EOS>',\n",
              " '<SOS> okay you are gonna need to learn how to lie <EOS>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avo6JJ4u0174"
      },
      "source": [
        "We also need to add tokens for `<SOS>` and `<EOS>` in our vocabulary, along with this two we will also be adding two more tokens: `<PAD>` and `<OUT>`, `<PAD>` token is used to represent the padding in the string(which will be provided further). `<OUT>` token represents those words(entered from user end) which will not be available in our dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0p_b1IhD9Eq"
      },
      "source": [
        "### Creating other necessary tokens\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "x = len(vocab)\n",
        "for token in tokens:\n",
        "  vocab[token]=x\n",
        "  x+=1\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9bsmsiMG7rL"
      },
      "source": [
        "\n",
        "### Setting index of <PAD> token as 0 \n",
        "vocab['cameron']= vocab['<PAD>']\n",
        "vocab['<PAD>']=0"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Greh6zHQ5w"
      },
      "source": [
        "# Inversing the dictionary\n",
        "inv_vocab = {w:v for v,w in vocab.items()}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnym9xy21yy4"
      },
      "source": [
        "### <b>Creating Encoder and Decoder Input for Training </b>\n",
        "The encoder-decoder model is a way of using recurrent neural networks for sequence-to-sequence prediction problems.<br>\n",
        "The overall most basic structure of sequence to sequence model(encoder-decoder) which is commonly used is as shown below-\n",
        "![alt text](https://miro.medium.com/max/1250/1*zq1G3mPSuy-KoMlBldohww.png)\n",
        "It consists of 3 parts:<b> encoder, intermediate vector</b> and <b>decoder.</b><br>\n",
        "<br>\n",
        "<b>Encoder-</b>It accepts a single element of the input sequence at each time step, process it, collects information for that element and propagates it forward.<br>\n",
        "<b>Intermediate vector-</b> This is the final internal state produced from the encoder part of the model. It contains information about the entire input sequence to help the decoder make accurate predictions.<br>\n",
        "<b>Decoder-</b> given the entire sentence, it predicts an output at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huOVgCL0H4Yb",
        "outputId": "a6bb416c-574a-4bd7-dafd-c74acd345554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Creating encoder input to feed the model\n",
        "encoder_inp = []\n",
        "for line in clean_ques:\n",
        "  lst=[]\n",
        "  for word in line.split():\n",
        "    if word not in vocab:\n",
        "      lst.append(vocab['<OUT>'])\n",
        "    else:\n",
        "      lst.append(vocab[word])\n",
        "  encoder_inp.append(lst)\n",
        "encoder_inp[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3467], [1], [2], [3, 4], [5], [6, 7], [8, 9], [10], [11, 12], [9]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aycAEaSHL64S",
        "outputId": "adf3e992-278e-4b25-c771-bc0c951c0deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "### Creating decoder input for answers\n",
        "decoder_inp = []\n",
        "for line in clean_ans:\n",
        "  lst=[]\n",
        "  for word in line.split():\n",
        "    if word not in vocab:\n",
        "      lst.append(vocab['<OUT>'])\n",
        "    else:\n",
        "      lst.append(vocab[word])\n",
        "  decoder_inp.append(lst)\n",
        "\n",
        "decoder_inp[5:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3470, 47, 33, 1175, 18, 31, 35, 209, 50, 111, 47, 3468],\n",
              " [3470, 85, 27, 28, 147, 47, 783, 18, 74, 3468],\n",
              " [3470, 47, 835, 1300, 46, 1638, 3468],\n",
              " [3470, 50, 46, 3469, 1581, 3469, 6, 40, 370, 1462, 516, 3468],\n",
              " [3470, 15, 47, 83, 2225, 1377, 18, 1943, 60, 18, 804, 3468]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LJM5O2e4u1_"
      },
      "source": [
        "#### <b>Padding</b>\n",
        "What does this mean? It means that we need to search for the length of the longest sentence, convert every sentence to a vector of that length, and fill the gap between the number of words of each sentence, and the number of words of the longest sentences with zeros.<br>\n",
        "Why Padding?We need padding because our model expects all\n",
        "the sentences in a batch whether they are questions or answers must have the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0adWJQM09E"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "### Padding our encoder and decoder values\n",
        "encoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\n",
        "decoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2pT1AkVOLCI",
        "outputId": "075bf93d-dc97-4e1a-8b3b-54c3988408da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "encoder_inp[:5]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3467,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [   2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [   3,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [   5,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl0zj8EOQ7gL",
        "outputId": "a24d8d0e-114e-4941-e925-c567ec941c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "decoder_inp[:5]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3470,   61,  538,   28, 3467,    6,   40,  285,   61, 3469,   69,\n",
              "          88, 2405],\n",
              "       [3470, 3469,  967,   14, 1077,   18,  230,  126, 2406,   71,   14,\n",
              "        2407,  647],\n",
              "       [3470,  152, 3468,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [3470,    6,  126,  126,  126, 1069,   31,   10,    6,  238,   29,\n",
              "          29, 1413],\n",
              "       [3470, 1551,  370,  911, 1018,   35,  106,   65, 3468,    0,    0,\n",
              "           0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iewe43ok6EZs"
      },
      "source": [
        "We created this new list of decoder as there is always a same token(3470) at the begining of every list, this is the token of `<SOS>` string, we need to remove this from our decoder model, otherwise the predicting model will always be predicting this at the start of every replied string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBsNGv_vOwGC",
        "outputId": "457eb49f-e726-4636-92a0-02d3d05047a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "final_decoder = []\n",
        "for inp in decoder_inp:\n",
        "  final_decoder.append(inp[1:])\n",
        "final_decoder = pad_sequences(final_decoder, 13, padding='post', truncating='post')\n",
        "final_decoder[:5]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  61,  538,   28, 3467,    6,   40,  285,   61, 3469,   69,   88,\n",
              "        2405,    0],\n",
              "       [3469,  967,   14, 1077,   18,  230,  126, 2406,   71,   14, 2407,\n",
              "         647,    0],\n",
              "       [ 152, 3468,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0],\n",
              "       [   6,  126,  126,  126, 1069,   31,   10,    6,  238,   29,   29,\n",
              "        1413,    0],\n",
              "       [1551,  370,  911, 1018,   35,  106,   65, 3468,    0,    0,    0,\n",
              "           0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULGTm2HFR5jM",
        "outputId": "252ec76f-1b02-4a45-edbe-8cc44c081aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.shape(final_decoder)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31416, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfmuRXpO61_5"
      },
      "source": [
        "<b> Reshaping Decoder Input</b><br>\n",
        "We need to convert our final_decoder data to 3D data, as our model expects 3D data for training.\n",
        "Here, we will we using `to_categorical()` function for this purpose, it converts a class vector (integers) to binary class matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfrUHfkOxRTm",
        "outputId": "d5c52cee-9959-4dd8-b29f-dc852f7e844a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "final_decoder = to_categorical(final_decoder, len(vocab))\n",
        "final_decoder.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31416, 13, 3471)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUJhupW071TX"
      },
      "source": [
        "## <b>Create Model</b>\n",
        "Here we will be using Recurrent Neural Networks.The RNN used here is <b>Long Short Term Memory(LSTM).</b> \n",
        "<br>\n",
        "Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies.\n",
        "<br>\n",
        "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior.\n",
        "\n",
        "The Encoder loops over the sequence. At each step it takes the encoded representation of a token and passes it through the LSTM cell, that also gets an input of the state of the sentence representation so far. When EOS is reached, we collect the final state of our cell (cell_state, hidden_state).\n",
        "![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/encoder.png)\n",
        "<br>\n",
        "<br>\n",
        "The Decoder shares the same basic architecture, with one added layer (here a perceptron layer) to predict the new token.\n",
        "![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CikiQP8Qxaz"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hymDzTcskH2F"
      },
      "source": [
        "### Creating input placeholders for our encoder and decoder\n",
        "enc_inp = Input(shape=(13,))\n",
        "dec_inp = Input(shape=(13,))\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udn0wjDA_p7M"
      },
      "source": [
        "#### <b>Word Embedding</b>\n",
        "Word embeddings are mathematical representations of words encoded as vectors in n-dimentional space. Similar words are close to each other in this space. This means that we can compare 2 or more words to each other not by e.g. the number of overlapping characters but by how close they are to each other in they embedded form.\n",
        "<br>\n",
        "<b>Why Word Embedding?</b><br>\n",
        "Our goal in designing an intent detection is to create a system that, given a few examples for intent, can detect that a sentence given by the user is similar to these examples and therefore should have the same intent.\n",
        "The problem behind this system is that we have to design a system for checking if 2 sentences are similar. This could be achieved by eg. counting how many overlapping words are in the new sentence and the sentences in training data set. This is however a naive approach because a user can use a word that has similar meaning, but is different from the ones in the train examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9idpZo4Tkf3D"
      },
      "source": [
        "### Creating embedding layer\n",
        "Vocab_Size=len(vocab)\n",
        "embed = Embedding(Vocab_Size+1, output_dim=70, input_length=13, trainable=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQSsugW1lgfQ"
      },
      "source": [
        "### Embedding encoder input\n",
        "enc_embed = embed(enc_inp)\n",
        "### Creating object for encoder LSTM Layer\n",
        "enc_lstm = LSTM(700, return_sequences=True, return_state=True)\n",
        "enc_op, h,c = enc_lstm(enc_embed)\n",
        "enc_states=[h,c]\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPN9inLm6mU"
      },
      "source": [
        "### Embedding decoder input\n",
        "dec_embed = embed(dec_inp)\n",
        "### Creating object for decoder LSTM Layer\n",
        "dec_lstm = LSTM(700, return_sequences=True, return_state=True)\n",
        "dec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5eLNHWyCivO"
      },
      "source": [
        "#### <b>Dense Layer</b>\n",
        "A Dense Layer is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. The layer has a weight matrix <b>W</b>, a bias vector <b>b</b>, and the activations of previous layer <b>a</b>. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-fXQbqpnml4"
      },
      "source": [
        "### Creating dense layer\n",
        "dense = Dense(Vocab_Size, activation='softmax')\n",
        "dense_output = dense(dec_op)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRODQ25Xn_19"
      },
      "source": [
        "### Creating and Compiling Model \n",
        "model = Model([enc_inp, dec_inp], dense_output)\n",
        "model.compile(loss='categorical_crossentropy',metrics=['acc'], optimizer='adam')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW2E25ZJrI-J",
        "outputId": "828b4a84-1102-4c91-b3f2-0a0f0059ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Vocab_Size"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3471"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyP7FI1Noif3",
        "outputId": "c3564abf-8cc1-498c-d159-ba756e714bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "### Fitting the data to our model\n",
        "model.fit([encoder_inp, decoder_inp], final_decoder, epochs=60)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 3.1775 - acc: 0.4808\n",
            "Epoch 2/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.7884 - acc: 0.5185\n",
            "Epoch 3/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.6633 - acc: 0.5267\n",
            "Epoch 4/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.5779 - acc: 0.5316\n",
            "Epoch 5/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.5008 - acc: 0.5356\n",
            "Epoch 6/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.4226 - acc: 0.5392\n",
            "Epoch 7/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.3369 - acc: 0.5433\n",
            "Epoch 8/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.2426 - acc: 0.5498\n",
            "Epoch 9/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.1401 - acc: 0.5583\n",
            "Epoch 10/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 2.0339 - acc: 0.5707\n",
            "Epoch 11/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.9283 - acc: 0.5866\n",
            "Epoch 12/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.8286 - acc: 0.6033\n",
            "Epoch 13/60\n",
            "982/982 [==============================] - 39s 39ms/step - loss: 1.7353 - acc: 0.6213\n",
            "Epoch 14/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.6486 - acc: 0.6391\n",
            "Epoch 15/60\n",
            "982/982 [==============================] - 39s 39ms/step - loss: 1.5693 - acc: 0.6564\n",
            "Epoch 16/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.4946 - acc: 0.6726\n",
            "Epoch 17/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.4253 - acc: 0.6889\n",
            "Epoch 18/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.3614 - acc: 0.7035\n",
            "Epoch 19/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 1.3027 - acc: 0.7178\n",
            "Epoch 20/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.2493 - acc: 0.7303\n",
            "Epoch 21/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.2000 - acc: 0.7421\n",
            "Epoch 22/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.1565 - acc: 0.7517\n",
            "Epoch 23/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.1158 - acc: 0.7607\n",
            "Epoch 24/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.0782 - acc: 0.7693\n",
            "Epoch 25/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.0451 - acc: 0.7763\n",
            "Epoch 26/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 1.0134 - acc: 0.7827\n",
            "Epoch 27/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.9880 - acc: 0.7881\n",
            "Epoch 28/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.9602 - acc: 0.7936\n",
            "Epoch 29/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.9352 - acc: 0.7981\n",
            "Epoch 30/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.9121 - acc: 0.8030\n",
            "Epoch 31/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.8909 - acc: 0.8069\n",
            "Epoch 32/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 0.8706 - acc: 0.8103\n",
            "Epoch 33/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.8516 - acc: 0.8137\n",
            "Epoch 34/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.8333 - acc: 0.8169\n",
            "Epoch 35/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.8149 - acc: 0.8208\n",
            "Epoch 36/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7984 - acc: 0.8236\n",
            "Epoch 37/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7830 - acc: 0.8264\n",
            "Epoch 38/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7663 - acc: 0.8300\n",
            "Epoch 39/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7519 - acc: 0.8325\n",
            "Epoch 40/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7379 - acc: 0.8350\n",
            "Epoch 41/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7240 - acc: 0.8375\n",
            "Epoch 42/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.7113 - acc: 0.8401\n",
            "Epoch 43/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 0.6966 - acc: 0.8429\n",
            "Epoch 44/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 0.6853 - acc: 0.8450\n",
            "Epoch 45/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.6759 - acc: 0.8465\n",
            "Epoch 46/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.6582 - acc: 0.8507\n",
            "Epoch 47/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.6493 - acc: 0.8520\n",
            "Epoch 48/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.6419 - acc: 0.8531\n",
            "Epoch 49/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 0.6266 - acc: 0.8571\n",
            "Epoch 50/60\n",
            "982/982 [==============================] - 40s 40ms/step - loss: 0.6184 - acc: 0.8583\n",
            "Epoch 51/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.6119 - acc: 0.8590\n",
            "Epoch 52/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5999 - acc: 0.8621\n",
            "Epoch 53/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5885 - acc: 0.8649\n",
            "Epoch 54/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5836 - acc: 0.8649\n",
            "Epoch 55/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5784 - acc: 0.8659\n",
            "Epoch 56/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5689 - acc: 0.8678\n",
            "Epoch 57/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5613 - acc: 0.8698\n",
            "Epoch 58/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5536 - acc: 0.8711\n",
            "Epoch 59/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5465 - acc: 0.8724\n",
            "Epoch 60/60\n",
            "982/982 [==============================] - 39s 40ms/step - loss: 0.5438 - acc: 0.8724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f79be9e1c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "strWCC9xJJKj",
        "outputId": "62d0359c-9074-4c8a-ec15-baf3f8b5275a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 13)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 13, 70)       243040      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 13, 700), (N 2158800     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 13, 700), (N 2158800     embedding[1][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 13, 3471)     2433171     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,993,811\n",
            "Trainable params: 6,993,811\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRo-OBgV1Jmu",
        "outputId": "f80b05bc-01a9-45ad-e9fc-593afacd713c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/my_model') "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxC_rIjG1uzL"
      },
      "source": [
        "# # my_model directory\n",
        "# !ls saved_model\n",
        "\n",
        "# # Contains an assets folder, saved_model.pb, and variables folder.\n",
        "# !ls saved_model/my_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zin8ffs_u-By"
      },
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6It08ST11Yn",
        "outputId": "7f88b86f-eb12-4cc7-f0d3-e76e94cafa4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "model = tf.keras.models.load_model('saved_model/my_model')\n",
        "\n",
        "# Check its architecture\n",
        "model.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 13)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 13, 70)       243040      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 13, 700), (N 2158800     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 13, 700), (N 2158800     embedding[1][0]                  \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 13, 3471)     2433171     lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 6,993,811\n",
            "Trainable params: 6,993,811\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cViFcgFxFC3f"
      },
      "source": [
        "##<b> Inference</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTz4MIUepU2T"
      },
      "source": [
        "### Creating encoder model \n",
        "enc_model = Model([enc_inp], enc_states)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh1svEQmgtkb"
      },
      "source": [
        "### Creating decoder model\n",
        "decoder_state_input_h = Input(shape=(700,))\n",
        "decoder_state_input_c = Input(shape=(700,))\n",
        "### Creating list of decoder h,c\n",
        "decoder_state_input = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3xrOILnDqu4"
      },
      "source": [
        "#### <b>How decoder creates new sentences?</b>\n",
        "The encoder provides a representation of the whole sequence. One option is to initialize the decoder state with this representation, then just send an SOS token to start the generation of the new sequence.<br>\n",
        "In order to provide to the decoder a maximum of information about its progression, we can pass the decoded token as the new input when generating the next one. We repeat the process until the decoder raises the EOS signal.<br>\n",
        "![alt text](https://mk0caiblog1h3pefaf7c.kinstacdn.com/wp-content/uploads/2018/07/Seq2Seq_inference-1-1920x453.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hPQEOe0hpkK",
        "outputId": "580499a9-b7ca-491a-a080-c1b97a054450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "decoder_outputs, state_h, state_c = dec_lstm(dec_embed, initial_state=decoder_state_input)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_model = Model([dec_inp]+decoder_state_input, [decoder_outputs]+decoder_states)\n",
        "print(\"#######################################################################\")\n",
        "print(\"#                       Starting Chatbot ver 1.0                      #\")\n",
        "print(\"#######################################################################\")\n",
        "\n",
        "### Creating user input condition\n",
        "prepro1= \"\"\n",
        "while prepro1 != 'q':\n",
        "  prepro1 = input(\"You: \")\n",
        "  # Cleaning the text\n",
        "  prepro1 = clean_text(prepro1)\n",
        "  prepro = [prepro1]\n",
        "\n",
        "  # Creating a list for processed text\n",
        "  new_txt = []\n",
        "  for x in prepro:\n",
        "    new_list = []\n",
        "    for y in x.split():\n",
        "      try:\n",
        "        new_list.append(vocab[y])\n",
        "      except:\n",
        "        new_list.append(vocab['<OUT>'])\n",
        "    new_txt.append(new_list)\n",
        "\n",
        "  # Applying pad to our new_txt list\n",
        "  new_txt = pad_sequences(new_txt, 13, padding='post')\n",
        "  # Predicting the txt input\n",
        "  stat = enc_model.predict(new_txt)\n",
        "\n",
        "  empty_target_seq = np.zeros( (1, 1) )\n",
        "  empty_target_seq[0,0] = vocab['<SOS>']\n",
        "\n",
        "  stop_condition = False\n",
        "  decoded_translation = ''\n",
        "\n",
        "  while not stop_condition:\n",
        "    decoder_outputs,h,c = decoder_model.predict([empty_target_seq]+stat)\n",
        "    decoder_concat_inp = dense(decoder_outputs)\n",
        "\n",
        "    sampled_word_index = np.argmax(decoder_concat_inp[0, -1, :])\n",
        "    sampled_word = inv_vocab[sampled_word_index]+ ' '\n",
        "\n",
        "    if sampled_word!= '<EOS> ':\n",
        "      decoded_translation += sampled_word\n",
        "    \n",
        "    if sampled_word == '<EOS> ' or len(decoded_translation.split())>13:\n",
        "      stop_condition=True\n",
        "  \n",
        "    ## Reseting the variabls\n",
        "    empty_target_seq = np.zeros( (1, 1) ) \n",
        "    empty_target_seq[0,0] = sampled_word_index\n",
        "    stat = [h,c]\n",
        "    # decoded_trans_out=''\n",
        "    # for word in decoded_translation:\n",
        "    #   if word!='<PAD>' or word!='<OUT>':\n",
        "    #     decoded_trans_out+= word\n",
        "\n",
        "\n",
        "  \n",
        "  print(\"Chatbot Attention: \", decoded_translation)\n",
        "  print(\"==============================================================\")\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#######################################################################\n",
            "#                       Starting Chatbot ver 1.0                      #\n",
            "#######################################################################\n",
            "You: hello\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 13) for input Tensor(\"input_2:0\", shape=(None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Chatbot Attention:  <OUT> is that you \n",
            "==============================================================\n",
            "You: yeah its me again\n",
            "Chatbot Attention:  i will be in my quarters if needed but i would prefer <PAD> <PAD> \n",
            "==============================================================\n",
            "You: ok\n",
            "Chatbot Attention:  you dont \n",
            "==============================================================\n",
            "You: ok\n",
            "Chatbot Attention:  you dont \n",
            "==============================================================\n",
            "You: are you a man\n",
            "Chatbot Attention:  i dont think so \n",
            "==============================================================\n",
            "You: then what are you\n",
            "Chatbot Attention:  i am sorry \n",
            "==============================================================\n",
            "You: why sorry\n",
            "Chatbot Attention:  i am not allowed to date \n",
            "==============================================================\n",
            "You: ooh are you committed?\n",
            "Chatbot Attention:  well its more than we had ten minutes ago \n",
            "==============================================================\n",
            "You: did you broke up\n",
            "Chatbot Attention:  no this happened to me \n",
            "==============================================================\n",
            "You: ohh thats amazing\n",
            "Chatbot Attention:  no you wouldnt that is a good thing \n",
            "==============================================================\n",
            "You: what really? is that good thing?\n",
            "Chatbot Attention:  <OUT> \n",
            "==============================================================\n",
            "You: tell me more about yourself\n",
            "Chatbot Attention:  i would just as soon kill you jesse james but chasing you <PAD> <PAD> \n",
            "==============================================================\n",
            "You: thats weird\n",
            "Chatbot Attention:  <OUT> \n",
            "==============================================================\n",
            "You: are you a woman\n",
            "Chatbot Attention:  i dont know \n",
            "==============================================================\n",
            "You: damn!\n",
            "Chatbot Attention:  i think it was about that you know the time is the <PAD> <PAD> \n",
            "==============================================================\n",
            "You: what are you talking about?\n",
            "Chatbot Attention:  its not like coming coming on \n",
            "==============================================================\n",
            "You: do you like me\n",
            "Chatbot Attention:  no i am not i am not \n",
            "==============================================================\n",
            "You: but why?\n",
            "Chatbot Attention:  because you remind me of all the dumb things i said \n",
            "==============================================================\n",
            "You: you are annoying\n",
            "Chatbot Attention:  yeah i am sure \n",
            "==============================================================\n",
            "You: i am going\n",
            "Chatbot Attention:  i am not kidding \n",
            "==============================================================\n",
            "You: yeah ok\n",
            "Chatbot Attention:  you are <OUT> man whens the last time you had some fun <PAD> <PAD> \n",
            "==============================================================\n",
            "You: dont remember\n",
            "Chatbot Attention:  i have enjoyed being man \n",
            "==============================================================\n",
            "You: wow thats great\n",
            "Chatbot Attention:  that is it \n",
            "==============================================================\n",
            "You: see you later\n",
            "Chatbot Attention:  its possible \n",
            "==============================================================\n",
            "You: yeah i knw it is\n",
            "Chatbot Attention:  no you dont have to tell the truth \n",
            "==============================================================\n",
            "You: ok i wont\n",
            "Chatbot Attention:  what \n",
            "==============================================================\n",
            "You: bye\n",
            "Chatbot Attention:  bye that was <OUT> \n",
            "==============================================================\n",
            "You: good bye\n",
            "Chatbot Attention:  i am totally serious \n",
            "==============================================================\n",
            "You: me too\n",
            "Chatbot Attention:  i will call you right back \n",
            "==============================================================\n",
            "You: ok\n",
            "Chatbot Attention:  you dont \n",
            "==============================================================\n",
            "You: ok i wont call you\n",
            "Chatbot Attention:  what \n",
            "==============================================================\n",
            "You: nothing leave it\n",
            "Chatbot Attention:  it will be fine you have another \n",
            "==============================================================\n",
            "You: bye\n",
            "Chatbot Attention:  bye that was <OUT> \n",
            "==============================================================\n",
            "You: q\n",
            "Chatbot Attention:  <OUT> \n",
            "==============================================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}